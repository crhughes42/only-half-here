---
title: "Homework Eight"
author: "Chelsea Hughes"
output:
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
---
 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
data1 <- read.delim("~/R/uscrime.txt")
library(knitr) 
library(stats)
library(ggplot2)
library(glmnet) 
library(DAAG)
library(tree)
library(caret)
set.seed(42)

#setup functions 
tree1 <- tree(Crime~., data1)
yhat1 <- predict(tree1)
crim1 <- data1$Crime
computeR2 <- function(yhat1, data1){
sse1 <- sum((yhat1 - crim1)^2)
sst1 <- sum((crim1 - mean(crim1))^2)
R2 <- 1 - sse1/sst1
return(R2)}

```
## Question 11.1 
**Using the crime data set uscrime.txt from Questions 8.2, 9.1, and 10.1, build a regression model using:1. Stepwise regression 2. Lasso 3. Elastic net. For Parts 2 and 3, remember to scale the data first – otherwise, the regression coefficients will be on different scales and the constraint won’t have the desired effect.**

## Stepwise Regression
```{r part 1, include=TRUE}

#linear regression with all factors
model1 <- lm(Crime ~., data1)
step(model1, direction = "both")

#train model with best factors
model2 <- lm(formula = Crime ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + Prob, data = data1)
summary(model2)

#cross validate
model3 <- cv.lm(data = data1, form.lm = model2, m = 8)
summary(model3)

#calculate R2 
yhat2 <- as.data.frame(model3$cvpred)
model3R2 <- computeR2(yhat2, data1)
model3R2
```
 
## Lasso  
**In a function call like glmnet(x,y,family=”gaussian”,alpha=1) the predictors x need to be in R’s matrix format, rather than data frame format. You can convert a data frame to a matrix using as.matrix – for example, x <- as.matrix(data[,1:n-1]). Rather than specifying a value of T, glmnet returns models for a variety of values of T.** 
```{r part 2, include=TRUE}

#scale the data
data2 <- as.data.frame(scale(data1[,c(1,3:15)]))
data2 <- cbind(data1[,2],data2,data1[,16])
colnames(data2)[1] <- "So"
colnames(data2)[16] <- "Crime"

#convert to matrix 
data3 <- as.matrix(data2)

#fit model 
fit = glmnet(x = data3[,-16], y = data3[,"Crime"]) 
plot(fit)

#use LASSO
lasso1 <- cv.glmnet(x = data3[,-16], y = data3[,"Crime"], alpha = 1, nfolds = 5,type.measure = "mse",family = "gaussian")

#visualize lambda.min factors
lasso1$lambda.min 

#coefficients for lambda.min
coeff1 <- coef(lasso1, s = lasso1$lambda.min)
coeff1

#train model with lambda.min factors
model4 <- lm(formula = Crime ~ So + M + Ed + Po1 + M.F + Pop + NW + U1 + U2 + Wealth + Ineq + Prob, data = data1)
summary(model4)

#cross validate
model5 <- cv.lm(data = data1, form.lm = model4, m = 8)
summary(model5)

#use glmnet to cross validate 
cvfit = cv.glmnet(x = data3[,-16], y = data3[,"Crime"])
plot(cvfit)
summary(cvfit)

#calculate R2
yhat2 <- as.data.frame(model5$cvpred)
model5R2 <- computeR2(yhat2, data1)
model5R2


```

## Elastic Net  
**For the elastic net model, what we called λ in the videos, glmnet calls “alpha”; you can get a range of results by varying alpha from 1 (lasso) to 0 (ridge regression) [and, of course, other values of alpha in between].** 
*Since Lasso (α = 1) was a required part of homework, I decided to see the variation of λ(videos)/α(glmnet) for every value between 0 ≤ α ≤ 1. In order, I tested in descending order: .75, .5, .25 and 0.* [1]  

# α: 0.75
```{r part 3.1, include=TRUE}

#Elastic Net
en1 <- cv.glmnet(x = data3[,-16], y = data3[,"Crime"], alpha = 0.75, nfolds = 5,type.measure = "mse",family = "gaussian")

#visualize lambda.min factors
en1$lambda.min

#coefficients for lambda.min
coeff2 <- coef(en1, s = en1$lambda.min)
coeff2

#train model with lambda.min factors
model6 <- lm(formula = Crime ~ So + M + Ed + Po1 + Po2 + LF + M.F + Pop + NW + U1 + U2 + Wealth + Ineq + Prob, data = data1)
summary(model6)

#cross validate
model7 <- cv.lm(data = data1, form.lm = model6, m = 8)
summary(model7)

#calculate R2
yhat3 <- as.data.frame(model7$cvpred)
model7R2 <- computeR2(yhat3, data1)
model7R2

```

# α: 0.5
```{r part 3.2, include=TRUE}

#Elastic Net
en2 <- cv.glmnet(x = data3[,-16], y = data3[,"Crime"], alpha = 0.5, nfolds = 5,
type.measure = "mse",family = "gaussian")

#visualize lambda.min factors
en2$lambda.min

#coefficients for lambda.min
coeff3 <- coef(en2, s = en2$lambda.min)
coeff3

#train model with lambda.min factors
model8 <- lm(formula = Crime ~ So + M + Ed + Po1 + Po2 + M.F + Pop + NW + U1 + U2 + Wealth + Ineq + Prob, data = data1)
summary(model8)

#cross validate
model9 <- cv.lm(data = data1, form.lm = model8, m = 8)
summary(model9)

#calculate R2
yhat4 <- as.data.frame(model9$cvpred)
model9R2 <- computeR2(yhat4, data1)
model9R2

```

# α: 0.25
```{r part 3.3, include=TRUE}

#Elastic Net
en3 <- cv.glmnet(x = data3[,-16], y = data3[,"Crime"], alpha = 0.25, nfolds = 5,type.measure = "mse",family = "gaussian")

#visualize lambda.min factors
en3$lambda.min

#coefficients for lambda.min
coeff4 <- coef(en3, s = en3$lambda.min)
coeff4

#train model with lambda.min factors
model10 <- lm(formula = Crime ~ So + M + Ed + Po1 + M.F + Pop + NW + U1 + U2 + Wealth + Ineq + Prob, data = data1)
summary(model10)

#cross validate
model11 <- cv.lm(data = data1, form.lm = model10, m = 8)
summary(model11)

#calculate R2
yhat5 <- as.data.frame(model11$cvpred)
model11R2 <- computeR2(yhat5, data1)
model11R2
```

# Ridge Regression
```{r, part 3.4, include=TRUE}

#Elastic Net
ridge1 <- cv.glmnet(x = data3[,-16], y = data3[,"Crime"], alpha = 0, nfolds = 5,type.measure = "mse",family = "gaussian")

#visualize lambda.min factors
ridge1$lambda.min

#coefficients for lambda.min
coeff5 <- coef(ridge1, s = ridge1$lambda.min)
coeff5

#train model with lambda.min factors
model12 <- lm(formula = Crime ~ So + M + Ed + Po1 + Po2 + LF + M.F + Pop + NW + U1 + U2 + Wealth + Ineq + Prob, data = data1)
summary(model12)

#cross validate
model13 <- cv.lm(data = data1, form.lm = model12, m = 8)
summary(model13)

#calculate R2
yhat6 <- as.data.frame(model13$cvpred)
model13R2 <- computeR2(yhat6, data1)
model13R2

```
For the second part, I did a couple versions of cross validation. As commentary, I preferred using the glmnet package for fitting and cross validating. I don't know that it would be my choice in real life, but for homework purposes... it was easier to visualize and decipher. For the third part, I kept all of the defaults in place, so for every model: *nlambda* is 100 and *weight* was 1 per observation. [2] 
 
## References 
[1][links]http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/#computing-elastic-net-regession  
[2][links]https://web.stanford.edu/~hastie/Papers/Glmnet_Vignette.pdf  